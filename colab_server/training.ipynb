{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten, Conv1D, GlobalAveragePooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import mixed_precision\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mixed Precision 설정 (A100 GPU 최적화)\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# 진행 상황 출력용 Callback 클래스\n",
        "class TQDMProgressBar(Callback):\n",
        "    def __init__(self, total_epochs):\n",
        "        super().__init__()\n",
        "        self.total_epochs = total_epochs\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.epoch_bar = tqdm(total=self.total_epochs, desc=f\"Epoch {epoch + 1}/{self.total_epochs}\", position=0, leave=False)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.epoch_bar.update(1)\n",
        "        self.epoch_bar.close()\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if hasattr(self, 'batch_bar'):\n",
        "            self.batch_bar.close()\n",
        "\n",
        "# JSON 데이터 로드 함수\n",
        "def load_data_from_json(json_file):\n",
        "    X = []\n",
        "    y = []\n",
        "    # JSON 파일 읽기\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        for frame in data:\n",
        "            X.append(frame[:-1])  # 마지막 값을 제외하고 키포인트 데이터로 사용\n",
        "            y.append(frame[-1])  # 마지막 값은 라벨로 사용\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 데이터 경로\n",
        "json_file = \"/content/data_val.json\"\n",
        "\n",
        "# JSON 데이터 로드\n",
        "X, y = load_data_from_json(json_file)\n",
        "\n",
        "# 데이터 전처리: 레이블 원-핫 인코딩\n",
        "y = to_categorical(y)\n",
        "\n",
        "# 데이터 분할 (개선된 방식)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42, shuffle=True)\n",
        "\n",
        "# TensorFlow Dataset 생성\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(64).prefetch(tf.data.AUTOTUNE).repeat()\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).shuffle(2000).batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).shuffle(1000).batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# 모델 불러오기 또는 새로 생성\n",
        "def load_model_with_path(model_path):\n",
        "    if os.path.exists(model_path):\n",
        "        return load_model(model_path)\n",
        "    else:\n",
        "        # MLP 모델 설계\n",
        "        model = Sequential([\n",
        "            Conv1D(128, kernel_size=3, activation='relu', input_shape=(12, 3), kernel_regularizer=l2(0.0004)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Conv1D(64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.0004)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Conv1D(32, kernel_size=3, activation='relu', kernel_regularizer=l2(0.0004)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Conv1D(16, kernel_size=3, activation='relu', kernel_regularizer=l2(0.0004)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            GlobalAveragePooling1D(),\n",
        "\n",
        "            Dense(y_train.shape[1], activation='softmax', dtype='float32')\n",
        "        ])\n",
        "\n",
        "        # 모델 컴파일\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "# 체크포인트 콜백 설정\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model.keras',  # 저장할 파일 이름 (.keras 형식)\n",
        "    monitor='val_accuracy',  # 모니터링할 지표\n",
        "    mode='max',  # 최대화할 지표 (val_accuracy가 최대일 때 저장)\n",
        "    save_best_only=True,  # 가장 좋은 모델만 저장\n",
        "    verbose=1  # 저장 시 로그 출력\n",
        ")\n",
        "\n",
        "# 학습 진행 상황 출력\n",
        "progress_bar = TQDMProgressBar(total_epochs=20)\n",
        "\n",
        "# 모델 저장 파일 경로\n",
        "model = load_model_with_path('/content/best_model.keras')\n",
        "\n",
        "# EarlyStopping 콜백 설정\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # 모니터링할 지표 (예: 'val_loss' 또는 'val_accuracy')\n",
        "    patience=5,          # 개선되지 않은 에포크 수 (5번 연속으로 개선되지 않으면 중지)\n",
        "    restore_best_weights=True,  # 가장 좋은 가중치를 복원\n",
        "    verbose=1            # 중지 시 로그 출력\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=100,  # 전체 데이터를 학습하지 않고 일부 데이터로 학습\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    callbacks=[progress_bar, checkpoint, early_stopping],  # 조기 종료 콜백 추가\n",
        ")\n",
        "\n",
        "# 모델 평가\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "-gikuS-1KkSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bfe94b6-f9af-4900-bdf5-c4a871e4ef45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m 85/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4915 - loss: 1.2771"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 0.20087, saving model to best_model.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step - accuracy: 0.5245 - loss: 1.2156 - val_accuracy: 0.2009 - val_loss: 1.8462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10\n",
            "\u001b[1m 84/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.8983 - loss: 0.4529"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: val_accuracy did not improve from 0.20087\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.8997 - loss: 0.4458 - val_accuracy: 0.2009 - val_loss: 2.1103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10\n",
            "\u001b[1m 96/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9335 - loss: 0.2961"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: val_accuracy improved from 0.20087 to 0.53558, saving model to best_model.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9338 - loss: 0.2952 - val_accuracy: 0.5356 - val_loss: 1.6612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10\n",
            "\u001b[1m 92/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9435 - loss: 0.2531"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: val_accuracy improved from 0.53558 to 0.68866, saving model to best_model.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9436 - loss: 0.2526 - val_accuracy: 0.6887 - val_loss: 0.8816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10\n",
            "\u001b[1m 95/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9569 - loss: 0.2105"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5: val_accuracy improved from 0.68866 to 0.94355, saving model to best_model.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9567 - loss: 0.2104 - val_accuracy: 0.9435 - val_loss: 0.2927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10\n",
            "\u001b[1m 90/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9543 - loss: 0.1978"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6: val_accuracy did not improve from 0.94355\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9544 - loss: 0.1978 - val_accuracy: 0.9427 - val_loss: 0.2466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10\n",
            "\u001b[1m 94/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9623 - loss: 0.1805"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7: val_accuracy improved from 0.94355 to 0.96416, saving model to best_model.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9621 - loss: 0.1809 - val_accuracy: 0.9642 - val_loss: 0.1647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10\n",
            "\u001b[1m 87/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9593 - loss: 0.1779"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8: val_accuracy improved from 0.96416 to 0.97905, saving model to best_model.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9599 - loss: 0.1763 - val_accuracy: 0.9790 - val_loss: 0.1157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10\n",
            "\u001b[1m 92/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9668 - loss: 0.1635"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9: val_accuracy did not improve from 0.97905\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9667 - loss: 0.1633 - val_accuracy: 0.9683 - val_loss: 0.1332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10\n",
            "\u001b[1m 94/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9700 - loss: 0.1535"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10: val_accuracy did not improve from 0.97905\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9700 - loss: 0.1532 - val_accuracy: 0.9771 - val_loss: 0.1323\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "\u001b[1m103/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9805 - loss: 0.1104"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9805 - loss: 0.1103\n",
            "Test Loss: 0.1103, Test Accuracy: 0.9803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Validation data distribution:\")\n",
        "print(np.bincount(np.argmax(y_val, axis=1)))\n",
        "\n",
        "print(\"Test data distribution:\")\n",
        "print(np.bincount(np.argmax(y_test, axis=1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkRhNPj95jS_",
        "outputId": "dce1e91d-7ff1-400d-bc31-0103709a45be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation data distribution:\n",
            "[455 448 431 459 457]\n",
            "Test data distribution:\n",
            "[535 511 474 473 507]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):  # 첫 10개의 데이터 확인\n",
        "    print(f\"Features: {X_val[i]}, Label: {y_val[i]}\")"
      ],
      "metadata": {
        "id": "_PFYFXp26LOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group_into_triplets(data):\n",
        "    transformed_data = []\n",
        "    for sample in data:\n",
        "        grouped = [sample[i:i+3] for i in range(0, len(sample) - len(sample) % 3, 3)]  # 3개씩 묶음\n",
        "        grouped.append(sample[-1])\n",
        "        transformed_data.append(grouped)\n",
        "    return transformed_data\n",
        "\n",
        "# 변환된 데이터\n",
        "with open('/content/data_val.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "transformed_data = group_into_triplets(data)\n",
        "\n",
        "# JSON 파일로 저장\n",
        "output_file = \"transformed_data.json\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(transformed_data, f, indent=4)"
      ],
      "metadata": {
        "id": "hCqVBnGTjinw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}